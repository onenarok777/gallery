"use client";

import { useEffect, useState, useRef } from "react";
import * as faceapi from "face-api.js";

interface DriveImage {
  id: string | null | undefined;
  name: string | null | undefined;
  src: string;
  originalLink: string | null | undefined;
  mimeType: string | null | undefined;
}

interface FaceSearchProps {
  allImages: DriveImage[];
  onMatchesFound: (matches: DriveImage[]) => void;
  onReset: () => void;
}

export default function FaceSearch({ allImages, onMatchesFound, onReset }: FaceSearchProps) {
  const [isModelLoaded, setIsModelLoaded] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [statusMessage, setStatusMessage] = useState("");
  const [referenceDescriptor, setReferenceDescriptor] = useState<Float32Array | null>(null);
  const [uploadedImageSrc, setUploadedImageSrc] = useState<string | null>(null);

  useEffect(() => {
    loadModels();
  }, []);

  const loadModels = async () => {
    try {
      setStatusMessage("Loading AI models...");
      const MODEL_URL = "/models";
      await Promise.all([
        faceapi.nets.ssdMobilenetv1.loadFromUri(MODEL_URL),
        faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
        faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL),
      ]);
      setIsModelLoaded(true);
      setStatusMessage("Ready to search");
    } catch (error) {
      console.error("Failed to load models", error);
      setStatusMessage("Error loading AI models");
    }
  };

  const handleImageUpload = async (e: React.ChangeEvent<HTMLInputElement>) => {
    if (!e.target.files?.length) return;

    const file = e.target.files[0];
    const url = URL.createObjectURL(file);
    setUploadedImageSrc(url);
    setIsProcessing(true);
    setStatusMessage("Analyzing uploaded face...");

    try {
      // 1. Get descriptor from uploaded image
      const img = await faceapi.fetchImage(url);
      const detection = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor();

      if (!detection) {
        setStatusMessage("No face detected in uploaded image. Please try another.");
        setIsProcessing(false);
        return;
      }

      const descriptor = detection.descriptor;
      setReferenceDescriptor(descriptor);
      
      // 2. Start scanning gallery
      await scanGallery(descriptor);

    } catch (error) {
      console.error(error);
      setStatusMessage("Error processing image");
    } finally {
      setIsProcessing(false);
    }
  };

  const scanGallery = async (refDescriptor: Float32Array) => {
    setStatusMessage(`Scanning ${allImages.length} images... (This may take a while)`);
    const matches: DriveImage[] = [];
    const faceMatcher = new faceapi.FaceMatcher(refDescriptor, 0.6); // 0.6 distance threshold

    for (let i = 0; i < allImages.length; i++) {
        const item = allImages[i];
        setStatusMessage(`Scanning ${i + 1}/${allImages.length}...`);
        
        try {
            // Use proxy to avoid CORS when fetching images for processing
            const proxyUrl = `/api/proxy-image?url=${encodeURIComponent(item.src)}`;
            
            // Note: fetchImage works with blob/image elements
            // We use standard HTML Image to load it first
            const img = await faceapi.fetchImage(proxyUrl);
            
            // Detect all faces in the gallery image
            const detections = await faceapi.detectAllFaces(img).withFaceLandmarks().withFaceDescriptors();
            
            // Check if any face matches
            const isMatch = detections.some(det => {
                const match = faceMatcher.findBestMatch(det.descriptor);
                return match.label !== 'unknown'; // 'person 1' (autogenerated by FaceMatcher for ref) matches
            });

            if (isMatch) {
                matches.push(item);
            }
        } catch (err) {
            console.warn(`Failed to scan image ${item.id}`, err);
        }
    }

    setStatusMessage(`Found ${matches.length} matches!`);
    onMatchesFound(matches);
  };

  return (
    <div className="p-4 mb-6 border rounded-lg bg-gray-50 dark:bg-zinc-900">
      <h3 className="text-lg font-semibold mb-2">Search by Face (Beta)</h3>
      
      {!isModelLoaded ? (
        <p className="text-yellow-600">{statusMessage}</p>
      ) : (
        <div className="flex flex-col gap-4">
          <div className="flex gap-4 items-start">
             <div>
                <input 
                    type="file" 
                    accept="image/*" 
                    onChange={handleImageUpload}  
                    disabled={isProcessing}
                    className="block w-full text-sm text-gray-500
                      file:mr-4 file:py-2 file:px-4
                      file:rounded-full file:border-0
                      file:text-sm file:font-semibold
                      file:bg-blue-50 file:text-blue-700
                      hover:file:bg-blue-100
                    "
                />
                <p className="text-sm text-gray-500 mt-1">{statusMessage}</p>
             </div>
             {uploadedImageSrc && (
                 <img src={uploadedImageSrc} className="h-20 w-20 object-cover rounded-md border" alt="Ref" />
             )}
          </div>
          
          {referenceDescriptor && !isProcessing && (
              <button 
                onClick={() => {
                    setUploadedImageSrc(null);
                    setReferenceDescriptor(null);
                    setStatusMessage("Ready");
                    onReset();
                }}
                className="text-sm text-red-500 hover:text-red-700 underline self-start"
              >
                Clear / Reset
              </button>
          )}
        </div>
      )}
    </div>
  );
}
